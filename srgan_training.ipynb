{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "NanOWNXLz6ah",
        "outputId": "f16b0282-ab69-4b03-90e4-c8a1af62a6d7"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe file 'venv\\lib\\site-packages\\typing_extensions.py' seems to be overriding built in modules and interfering with the startup of the kernel. Consider renaming the file and starting the kernel again.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresOverridingBuiltInModules'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# ISR is already installed - skip this cell if already installed\n",
        "# Uncomment below if you need to reinstall ISR\n",
        "# !pip install ISR==2.2.0 --no-deps\n",
        "# !pip install imageio tqdm pyaml\n",
        "\n",
        "# Suppress typing_extensions warning for kernel\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='typing_extensions')\n",
        "\n",
        "print(\"ISR is already installed. Skipping installation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "EzVd0Vp3z9tp",
        "outputId": "79a7e540-b0b2-4fb8-e9c8-12a1e01c559d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "# Create div2k directory if it doesn't exist\n",
        "os.makedirs('div2k', exist_ok=True)\n",
        "\n",
        "# Download DIV2K dataset files\n",
        "dataset_files = {\n",
        "    'DIV2K_train_LR_bicubic_X2.zip': 'http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X2.zip',\n",
        "    'DIV2K_valid_LR_bicubic_X2.zip': 'http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X2.zip',\n",
        "    'DIV2K_train_HR.zip': 'http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip',\n",
        "    'DIV2K_valid_HR.zip': 'http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip'\n",
        "}\n",
        "\n",
        "for filename, url in dataset_files.items():\n",
        "    filepath = os.path.join('div2k', filename)\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f'Downloading {filename}...')\n",
        "        try:\n",
        "            urllib.request.urlretrieve(url, filepath)\n",
        "            print(f'✓ {filename} downloaded successfully')\n",
        "        except Exception as e:\n",
        "            print(f'✗ Error downloading {filename}: {e}')\n",
        "    else:\n",
        "        print(f'✓ {filename} already exists')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-yMRyZB80MRt"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Extract all zip files in div2k directory\n",
        "zip_files = [\n",
        "    'div2k/DIV2K_train_LR_bicubic_X2.zip',\n",
        "    'div2k/DIV2K_valid_LR_bicubic_X2.zip',\n",
        "    'div2k/DIV2K_train_HR.zip',\n",
        "    'div2k/DIV2K_valid_HR.zip'\n",
        "]\n",
        "\n",
        "for zip_path in zip_files:\n",
        "    if os.path.exists(zip_path):\n",
        "        print(f'Extracting {zip_path}...')\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall('div2k')\n",
        "            print(f'✓ {zip_path} extracted successfully')\n",
        "        except Exception as e:\n",
        "            print(f'✗ Error extracting {zip_path}: {e}')\n",
        "    else:\n",
        "        print(f'✗ {zip_path} not found, skipping extraction')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NZkzHAKR0Uw0"
      },
      "outputs": [],
      "source": [
        "from ISR.models import RRDN\n",
        "from ISR.models import Discriminator\n",
        "from ISR.models import Cut_VGG19\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "colab_type": "code",
        "id": "tdadcpGS3rZy",
        "outputId": "0a13da76-508e-4384-ee7f-fb77cab86426"
      },
      "outputs": [],
      "source": [
        "lr_train_patch_size = 50\n",
        "layers_to_extract = [5, 9]\n",
        "scale = 4  # Changed to 4 to match your X4 LR images (your dataset has X4, not X2)\n",
        "hr_train_patch_size = lr_train_patch_size * scale\n",
        "\n",
        "rrdn  = RRDN(arch_params={'C':4, 'D':3, 'G':64, 'G0':64, 'T':10, 'x':scale}, patch_size=lr_train_patch_size)\n",
        "f_ext = Cut_VGG19(patch_size=hr_train_patch_size, layers_to_extract=layers_to_extract)\n",
        "discr = Discriminator(patch_size=hr_train_patch_size, kernel_size=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-irqXCW73w5R"
      },
      "outputs": [],
      "source": [
        "from ISR.train import Trainer\n",
        "import os\n",
        "\n",
        "loss_weights = {\n",
        "  'generator': 0.0,\n",
        "  'feature_extractor': 0.0833,\n",
        "  'discriminator': 0.01\n",
        "}\n",
        "losses = {\n",
        "  'generator': 'mae',\n",
        "  'feature_extractor': 'mse',\n",
        "  'discriminator': 'binary_crossentropy'\n",
        "} \n",
        "\n",
        "log_dirs = {'logs': './logs', 'weights': './weights'}\n",
        "\n",
        "learning_rate = {'initial_value': 0.0004, 'decay_factor': 0.5, 'decay_frequency': 30}\n",
        "\n",
        "flatness = {'min': 0.0, 'max': 0.15, 'increase': 0.01, 'increase_frequency': 5}\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET PATHS - Automatically configured based on your folder structure\n",
        "# ============================================================================\n",
        "# Based on your actual folder structure on E: drive\n",
        "\n",
        "base_path = 'E:\\\\'\n",
        "\n",
        "# Training dataset paths (configured from your actual folders)\n",
        "# NOTE: Your Train LR folder is X4, but model expects X2. You may need to adjust scale parameter\n",
        "# or find/create X2 LR images for proper training\n",
        "lr_train_path = os.path.join(base_path, 'DIV2K_train_LR_bicubic_X4') + os.sep  # Currently X4, needs X2 for scale=2\n",
        "hr_train_path = os.path.join(base_path, 'DIV2K_train_HR_bicubic_X2', 'DIV2K_train_HR_bicubic', 'X2') + os.sep\n",
        "\n",
        "# Validation dataset paths\n",
        "lr_valid_path = os.path.join(base_path, 'DIV2K_valid_LR_bicubic', 'X4') + os.sep  # Currently X4, needs X2 for scale=2\n",
        "hr_valid_path = os.path.join(base_path, 'DIV2K_valid_HR') + os.sep\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTANT: You have X4 LR images but the model is configured for X2 scale\n",
        "# Options:\n",
        "# 1. Change scale parameter to 4 (in Cell 4: scale = 4)\n",
        "# 2. Or download/get X2 LR images and update paths above\n",
        "# ============================================================================\n",
        "\n",
        "# Verify all paths exist\n",
        "print(\"Checking dataset directories...\")\n",
        "print(f\"Base path: {base_path}\\n\")\n",
        "\n",
        "paths_to_check = [\n",
        "    ('Train LR', lr_train_path),\n",
        "    ('Train HR', hr_train_path),\n",
        "    ('Valid LR', lr_valid_path),\n",
        "    ('Valid HR', hr_valid_path)\n",
        "]\n",
        "\n",
        "all_paths_exist = True\n",
        "for name, path in paths_to_check:\n",
        "    if path and os.path.exists(path.rstrip(os.sep)):\n",
        "        print(f'✓ Found: {name}')\n",
        "        print(f'  Path: {path}')\n",
        "    else:\n",
        "        print(f'✗ Missing: {name}')\n",
        "        print(f'  Path: {path if path else \"Not set\"}')\n",
        "        all_paths_exist = False\n",
        "\n",
        "if not all_paths_exist:\n",
        "    print(\"\\n⚠️  ERROR: Some dataset directories are missing!\")\n",
        "    print(\"\\nPlease update the paths at the top of this cell to match your dataset locations.\")\n",
        "    print(\"\\nRequired folders:\")\n",
        "    print(\"  1. Training LR (Low Resolution) images folder\")\n",
        "    print(\"  2. Training HR (High Resolution) images folder\")\n",
        "    print(\"  3. Validation LR images folder\")\n",
        "    print(\"  4. Validation HR images folder\")\n",
        "    raise FileNotFoundError(\"Dataset directories not found. Please update the paths in this cell.\")\n",
        "\n",
        "print(\"\\n✓ All dataset paths verified!\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    generator=rrdn,\n",
        "    discriminator=discr,\n",
        "    feature_extractor=f_ext,\n",
        "    lr_train_dir=lr_train_path,\n",
        "    hr_train_dir=hr_train_path,\n",
        "    lr_valid_dir=lr_valid_path,\n",
        "    hr_valid_dir=hr_valid_path,\n",
        "    loss_weights=loss_weights,\n",
        "    learning_rate=learning_rate,\n",
        "    flatness=flatness,\n",
        "    dataname='div2k',\n",
        "    log_dirs=log_dirs,\n",
        "    weights_generator=None,\n",
        "    weights_discriminator=None,\n",
        "    n_validation=50,\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Trainer initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "colab_type": "code",
        "id": "CfwekjDw31_I",
        "outputId": "cd7ad825-d472-446e-a6ba-77207cdb4e23"
      },
      "outputs": [],
      "source": [
        "trainer.train(\n",
        "    epochs=2,\n",
        "    steps_per_epoch=20,\n",
        "    batch_size=4,\n",
        "    monitored_metrics={'val_generator_PSNR_Y': 'max'}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "suftq9W9Zolg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "srgan_training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
